{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b34b4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769e8869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/29 19:03:34 WARN Utils: Your hostname, DESKTOP-FVKCA6T, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/29 19:03:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/29 19:03:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is ready\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HM-EDA\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f20446",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"file:///home/bartek/real-time-recommendation/data\"\n",
    "\n",
    "articles_path = f\"{data_path}/articles.csv\"\n",
    "customers_path = f\"{data_path}/customers.csv\"\n",
    "transactions_path = f\"{data_path}/transactions_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4a6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "articles = spark.read.csv(articles_path, header=True, inferSchema=True)\n",
    "customers = spark.read.csv(customers_path, header=True, inferSchema=True)\n",
    "transactions = spark.read.csv(transactions_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3439fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "articles.write.parquet(f\"{data_path}/articles.parquet\")\n",
    "customers.write.parquet(f\"{data_path}/customers.parquet\")\n",
    "transactions.write.parquet(f\"{data_path}/transactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc893d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = spark.read.parquet(f\"{data_path}/articles.parquet\")\n",
    "customers = spark.read.parquet(f\"{data_path}/customers.parquet\")\n",
    "transactions = spark.read.parquet(f\"{data_path}/transactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f429d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- t_dat: date (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- article_id: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- sales_channel_id: integer (nullable = true)\n",
      "\n",
      "+----------+--------------------+----------+--------------------+----------------+\n",
      "|     t_dat|         customer_id|article_id|               price|sales_channel_id|\n",
      "+----------+--------------------+----------+--------------------+----------------+\n",
      "|2019-11-29|aaa78c87aacba903d...| 706016003|0.025288135593220337|               2|\n",
      "|2019-11-29|aaa78c87aacba903d...| 706016001|0.025288135593220337|               2|\n",
      "|2019-11-29|aaa78c87aacba903d...| 682236013|0.018983050847457626|               2|\n",
      "|2019-11-29|aaa78c87aacba903d...| 706016016|0.025288135593220337|               2|\n",
      "|2019-11-29|aaa7a0483dd5b9e39...| 783335003|0.020322033898305086|               2|\n",
      "+----------+--------------------+----------+--------------------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "transactions.printSchema()\n",
    "\n",
    "transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f72a6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions: 31788324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique customers: 1362281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows = transactions.count()\n",
    "print(f\"Number of transactions: {total_rows}\")\n",
    "\n",
    "unique_customers = transactions.select(\"customer_id\").distinct().count()\n",
    "print(f\"Number of unique customers: {unique_customers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac639591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transactions2 = spark.read.csv(transactions_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aeec5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions: 31788324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:===================================================>    (24 + 2) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique customers: 1362281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows2 = transactions2.count()\n",
    "print(f\"Number of transactions: {total_rows2}\")\n",
    "\n",
    "unique_customers2 = transactions2.select(\"customer_id\").distinct().count()\n",
    "print(f\"Number of unique customers: {unique_customers2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5839cdf6",
   "metadata": {},
   "source": [
    "well... quite of a time difference between parquet and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae8c7cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 19:16:08 WARN DAGScheduler: Broadcasting large task binary with size 114.4 MiB\n",
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+--------+\n",
      "|         customer_id| user_idx|article_id|item_idx|\n",
      "+--------------------+---------+----------+--------+\n",
      "|aaa78c87aacba903d...| 340255.0| 706016003|     9.0|\n",
      "|aaa78c87aacba903d...| 340255.0| 706016001|     0.0|\n",
      "|aaa78c87aacba903d...| 340255.0| 682236013| 17656.0|\n",
      "|aaa78c87aacba903d...| 340255.0| 706016016|  6159.0|\n",
      "|aaa7a0483dd5b9e39...|1318380.0| 783335003| 44019.0|\n",
      "+--------------------+---------+----------+--------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_indexer = StringIndexer(inputCol=\"customer_id\", outputCol=\"user_idx\").setHandleInvalid(\"keep\")\n",
    "item_indexer = StringIndexer(inputCol=\"article_id\", outputCol=\"item_idx\").setHandleInvalid(\"keep\")\n",
    "\n",
    "pipeline = Pipeline(stages=[user_indexer, item_indexer])\n",
    "indexer_model = pipeline.fit(transactions)\n",
    "transactions_indexed = indexer_model.transform(transactions)\n",
    "\n",
    "transactions_indexed.select(\"customer_id\", \"user_idx\", \"article_id\", \"item_idx\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38168f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+-----------------+-----+\n",
      "|article_id|prod_name                |product_type_name|count|\n",
      "+----------+-------------------------+-----------------+-----+\n",
      "|706016001 |Jade HW Skinny Denim TRS |Trousers         |50287|\n",
      "|706016002 |Jade HW Skinny Denim TRS |Trousers         |35043|\n",
      "|372860001 |7p Basic Shaftless       |Socks            |31718|\n",
      "|610776002 |Tilly (1)                |T-shirt          |30199|\n",
      "|759871002 |Tilda tank               |Vest top         |26329|\n",
      "|464297007 |Greta Thong Mynta Low 3p |Underwear bottom |25025|\n",
      "|372860002 |7p Basic Shaftless       |Socks            |24458|\n",
      "|610776001 |Tilly (1)                |T-shirt          |22451|\n",
      "|399223001 |Curvy Jeggings HW Ankle  |Trousers         |22236|\n",
      "|706016003 |Jade HW Skinny Denim TRS |Trousers         |21241|\n",
      "|720125001 |SUPREME RW tights        |Leggings/Tights  |21063|\n",
      "|156231001 |Box 4p Tights            |Underwear Tights |21013|\n",
      "|562245046 |Luna skinny RW           |Trousers         |20719|\n",
      "|562245001 |Luna skinny RW           |Trousers         |20464|\n",
      "|351484002 |Lazer Razer Brief        |Swimwear bottom  |20415|\n",
      "|399256001 |Skinny Ankle R.W Brooklyn|Trousers         |20242|\n",
      "|673396002 |Ringo hipbelt            |Belt             |19834|\n",
      "|568601006 |Mariette Blazer          |Blazer           |19379|\n",
      "|448509014 |Perrie Slim Mom Denim TRS|Trousers         |19216|\n",
      "|673677002 |Henry polo. (1)          |Sweater          |19143|\n",
      "+----------+-------------------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "popular_products = transactions_indexed.groupBy(\"article_id\").count().orderBy(F.desc(\"count\")).limit(20)\n",
    "\n",
    "popular_products_with_names = popular_products.join(articles, \"article_id\").select(\"article_id\", \"prod_name\", \"product_type_name\", \"count\") \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "popular_products_with_names.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b1fdcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 19:20:57 WARN DAGScheduler: Broadcasting large task binary with size 117.0 MiB\n",
      "25/12/29 19:21:04 WARN DAGScheduler: Broadcasting large task binary with size 117.0 MiB\n",
      "25/12/29 19:21:07 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "[Stage 84:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-item matrix sparsity:\n",
      "Matrix is: 0.019173% full\n",
      "Sparsity: 99.980827%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 19:21:09 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "numerator = transactions_indexed.select(\"customer_id\", \"article_id\").distinct().count()\n",
    "num_users = transactions_indexed.select(\"user_idx\").distinct().count()\n",
    "num_items = transactions_indexed.select(\"item_idx\").distinct().count()\n",
    "\n",
    "denominator = num_users * num_items\n",
    "sparsity = (1.0 - (numerator * 1.0 / denominator)) * 100\n",
    "\n",
    "print('User-item matrix sparsity:')\n",
    "print(f\"Matrix is: {100 - sparsity:.6f}% full\")\n",
    "print(f\"Sparsity: {sparsity:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3cffd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = transactions_indexed.randomSplit([0.8, 0.2], seed=42)\n",
    "# we need to add a rating column with constant value of 1.0 for implicit feedback\n",
    "train = train.withColumn(\"rating\", F.lit(1.0))\n",
    "test = test.withColumn(\"rating\", F.lit(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbb4506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 19:32:29 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:32:33 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:32:48 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:32:54 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:00 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:04 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:09 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/12/29 19:33:12 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:15 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:19 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:22 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:25 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:29 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:33 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:35 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:40 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:43 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:46 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:49 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:53 WARN DAGScheduler: Broadcasting large task binary with size 121.4 MiB\n",
      "25/12/29 19:33:56 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:00 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:03 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:07 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:10 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:13 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:16 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:20 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:22 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:26 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:29 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:33 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:36 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:39 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:42 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:46 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:49 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:53 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:34:56 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:35:00 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:35:03 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:35:06 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:35:09 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:35:13 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:35:16 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:35:19 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:35:22 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:35:26 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully\n"
     ]
    }
   ],
   "source": [
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol=\"user_idx\", \n",
    "    itemCol=\"item_idx\", \n",
    "    ratingCol=\"rating\", \n",
    "    implicitPrefs=True,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    ")\n",
    "\n",
    "model = als.fit(train)\n",
    "print(\"Model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d78ff6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 19:38:03 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:43:03 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "[Stage 461:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------------------------------------------+\n",
      "|user_idx|recommendations                                                                       |\n",
      "+--------+--------------------------------------------------------------------------------------+\n",
      "|28      |[{3, 0.38629615}, {7, 0.3730123}, {49, 0.3720497}, {45, 0.37145802}, {57, 0.32911897}]|\n",
      "|31      |[{0, 0.5641865}, {4, 0.4799269}, {1, 0.460655}, {18, 0.4452774}, {8, 0.40456653}]     |\n",
      "|34      |[{4, 0.440464}, {18, 0.39387068}, {22, 0.35760227}, {33, 0.34230834}, {8, 0.2984539}] |\n",
      "|53      |[{3, 3.7996624}, {7, 3.2492824}, {40, 2.2350845}, {42, 2.0833542}, {32, 2.0828722}]   |\n",
      "|65      |[{4, 0.72568905}, {18, 0.6584402}, {0, 0.5223078}, {8, 0.5037251}, {50, 0.46586284}]  |\n",
      "+--------+--------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_recs = model.recommendForAllUsers(5)\n",
    "user_recs.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6eb4cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_indexed = indexer_model.stages[1].transform(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78c31cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 19:46:59 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 19:47:00 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/29 19:52:05 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "[Stage 511:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------------------+----------+\n",
      "|user_idx|article_id|prod_name                   |rating    |\n",
      "+--------+----------+----------------------------+----------+\n",
      "|28      |610776002 |Tilly (1)                   |0.38629615|\n",
      "|28      |610776001 |Tilly (1)                   |0.3730123 |\n",
      "|28      |678942001 |Harrison short sleeve top CN|0.3720497 |\n",
      "|28      |749699002 |Chestnut strap top          |0.37145802|\n",
      "|28      |749699001 |Chestnut strap top          |0.32911897|\n",
      "|31      |706016001 |Jade HW Skinny Denim TRS    |0.5641865 |\n",
      "|31      |759871002 |Tilda tank                  |0.4799269 |\n",
      "|31      |706016002 |Jade HW Skinny Denim TRS    |0.460655  |\n",
      "|31      |448509014 |Perrie Slim Mom Denim TRS   |0.4452774 |\n",
      "|31      |399223001 |Curvy Jeggings HW Ankle     |0.40456653|\n",
      "|34      |759871002 |Tilda tank                  |0.440464  |\n",
      "|34      |448509014 |Perrie Slim Mom Denim TRS   |0.39387068|\n",
      "|34      |160442007 |3p Sneaker Socks            |0.35760227|\n",
      "|34      |160442010 |3p Sneaker Socks            |0.34230834|\n",
      "|34      |399223001 |Curvy Jeggings HW Ankle     |0.2984539 |\n",
      "|53      |610776002 |Tilly (1)                   |3.7996624 |\n",
      "|53      |610776001 |Tilly (1)                   |3.2492824 |\n",
      "|53      |572797001 |ESSENTIAL TANKTOP LACE TVP  |2.2350845 |\n",
      "|53      |565379001 |Moa tank                    |2.0833542 |\n",
      "|53      |507909001 |Rebecca or Delphine shirt   |2.0828722 |\n",
      "|65      |759871002 |Tilda tank                  |0.72568905|\n",
      "|65      |448509014 |Perrie Slim Mom Denim TRS   |0.6584402 |\n",
      "|65      |706016001 |Jade HW Skinny Denim TRS    |0.5223078 |\n",
      "|65      |399223001 |Curvy Jeggings HW Ankle     |0.5037251 |\n",
      "|65      |759871001 |Tilda tank                  |0.46586284|\n",
      "+--------+----------+----------------------------+----------+\n",
      "only showing top 25 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recs_exploded = user_recs.withColumn(\"rec\", explode(\"recommendations\")).select(\"user_idx\", \"rec.item_idx\", \"rec.rating\")\n",
    "\n",
    "recs_with_info = recs_exploded.join(articles_indexed, \"item_idx\").select(\"user_idx\", \"article_id\", \"prod_name\", \"rating\")\n",
    "\n",
    "recs_with_info.show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed0b79cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 19:55:19 WARN DAGScheduler: Broadcasting large task binary with size 121.7 MiB\n",
      "25/12/29 19:55:24 WARN DAGScheduler: Broadcasting large task binary with size 121.7 MiB\n",
      "25/12/29 19:55:27 WARN TaskSetManager: Stage 562 contains a task of very large size (96429 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/29 19:55:28 WARN TaskSetManager: Stage 564 contains a task of very large size (2484 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"file:///home/bartek/real-time-recommendation/models/als_model\")\n",
    "indexer_model.save(\"file:///home/bartek/real-time-recommendation/models/indexer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "929691a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 19:59:51 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/29 19:59:52 WARN DAGScheduler: Broadcasting large task binary with size 121.7 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "item_factors = model.itemFactors\n",
    "\n",
    "item_vectors_to_export = item_factors.join(articles_indexed, item_factors.id == articles_indexed.item_idx) \\\n",
    "    .select(\"article_id\", \"prod_name\", \"features\")\n",
    "\n",
    "item_vectors_to_export.write.mode(\"overwrite\").parquet(\"file:///home/bartek/real-time-recommendation/data/processed/item_vectors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "651bc7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 20:00:33 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "[Stage 634:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |features                                                                                                                    |\n",
      "+---+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |[0.18760663, 0.05007008, 0.038590755, 0.03752847, 0.04380038, 0.046046425, 1.8137769, 0.061762534, 0.016788663, 0.091892295]|\n",
      "|10 |[0.6114376, 0.04562196, 0.038101897, 0.03688076, 0.040458187, 0.24343899, 0.04603544, 0.33030567, 0.031573664, 0.096841425] |\n",
      "|20 |[0.088714376, 0.009996864, 0.0033170704, 0.70789546, 0.015505002, 0.26316994, 0.0, 0.024749385, 0.018888019, 0.3161598]     |\n",
      "|30 |[0.0015739873, 0.008382113, 0.0, 0.036186825, 0.966362, 0.062153075, 0.02325028, 0.0, 0.0, 0.041700616]                     |\n",
      "|40 |[0.0, 0.9354742, 0.012773889, 0.021644885, 0.0, 0.0, 0.0, 0.0398406, 0.0, 0.098846376]                                      |\n",
      "+---+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "item_factors.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffec7b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 20:02:46 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 20:02:46 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "25/12/29 20:07:35 WARN DAGScheduler: Broadcasting large task binary with size 121.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recs_with_info.write.mode(\"overwrite\").parquet(\"file:///home/bartek/real-time-recommendation/data/processed/batch_recommendations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08a9f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 20:09:53 WARN DAGScheduler: Broadcasting large task binary with size 121.5 MiB\n",
      "25/12/29 20:09:55 WARN DAGScheduler: Broadcasting large task binary with size 117.0 MiB\n",
      "25/12/29 20:10:04 WARN DAGScheduler: Broadcasting large task binary with size 117.0 MiB\n",
      "25/12/29 20:10:10 WARN DAGScheduler: Broadcasting large task binary with size 121.7 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_factors = model.userFactors\n",
    "\n",
    "user_map = transactions_indexed.select(\"customer_id\", \"user_idx\").distinct()\n",
    "\n",
    "user_vectors_to_export = user_factors.join(user_map, user_factors.id == user_map.user_idx).select(\"customer_id\", \"features\")\n",
    "\n",
    "user_vectors_to_export.write.mode(\"overwrite\").parquet(\"file:///home/bartek/real-time-recommendation/data/processed/user_vectors.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real-time-recommendation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
